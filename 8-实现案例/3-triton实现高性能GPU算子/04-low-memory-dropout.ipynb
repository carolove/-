{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tabulate\n",
    "import torch\n",
    "\n",
    "import triton\n",
    "import triton.language as tl\n",
    "\n",
    "\n",
    "@triton.jit\n",
    "def _dropout(\n",
    "    x_ptr,  # pointer to the input\n",
    "    x_keep_ptr,  # pointer to a mask of 0s and 1s\n",
    "    output_ptr,  # pointer to the output\n",
    "    n_elements,  # number of elements in the `x` tensor\n",
    "    p,  # probability that an element of `x` is changed to zero\n",
    "    BLOCK_SIZE: tl.constexpr,\n",
    "):\n",
    "    pid = tl.program_id(axis=0)\n",
    "    block_start = pid * BLOCK_SIZE\n",
    "    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n",
    "    mask = offsets < n_elements\n",
    "    # Load data\n",
    "    x = tl.load(x_ptr + offsets, mask=mask)\n",
    "    x_keep = tl.load(x_keep_ptr + offsets, mask=mask)\n",
    "    # The line below is the crucial part, described in the paragraph above!\n",
    "    output = tl.where(x_keep, x / (1 - p), 0.0)\n",
    "    # Write-back output\n",
    "    tl.store(output_ptr + offsets, output, mask=mask)\n",
    "\n",
    "\n",
    "def dropout(x, x_keep, p):\n",
    "    output = torch.empty_like(x)\n",
    "    assert x.is_contiguous()\n",
    "    n_elements = x.numel()\n",
    "    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']), )\n",
    "    _dropout[grid](x, x_keep, output, n_elements, p, BLOCK_SIZE=1024)\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------  ---------  ---------  ---------  --------  ---------  ---------  -------  -------  --------  --------\n",
      "input      0.0139352  -0.340698  -0.770117  -1.65473  -0.268243  -0.276861  1.3833   1.18639  -1.0377   0.310245\n",
      "keep mask  1           1          0          0         1          1         1        0         1        1\n",
      "output     0.0278704  -0.681395   0          0        -0.536486  -0.553723  2.76659  0        -2.07541  0.62049\n",
      "---------  ---------  ---------  ---------  --------  ---------  ---------  -------  -------  --------  --------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Input tensor\n",
    "x = torch.randn(size=(10, )).cuda()\n",
    "# Dropout mask\n",
    "p = 0.5\n",
    "x_keep = (torch.rand(size=(10, )) > p).to(torch.int32).cuda()\n",
    "#\n",
    "output = dropout(x, x_keep=x_keep, p=p)\n",
    "print(tabulate.tabulate([\n",
    "    [\"input\"] + x.tolist(),\n",
    "    [\"keep mask\"] + x_keep.tolist(),\n",
    "    [\"output\"] + output.tolist(),\n",
    "]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------  ---------  --------  --------  -------  --------  --------  ----------  ---------  --------  --------\n",
      "input                -0.569103  0.406564  0.678731  1.04952  0.131175  0.697892  -0.0154628  -0.564523  0.689559  0.601873\n",
      "output (seed = 123)   0         0.813128  0         0        0         1.39578    0           0         1.37912   1.20375\n",
      "output (seed = 123)   0         0.813128  0         0        0         1.39578    0           0         1.37912   1.20375\n",
      "output (seed = 512)   0         0         1.35746   2.09904  0         1.39578   -0.0309255   0         0         0\n",
      "-------------------  ---------  --------  --------  -------  --------  --------  ----------  ---------  --------  --------\n"
     ]
    }
   ],
   "source": [
    "@triton.jit\n",
    "def _seeded_dropout(\n",
    "    x_ptr,\n",
    "    output_ptr,\n",
    "    n_elements,\n",
    "    p,\n",
    "    seed,\n",
    "    BLOCK_SIZE: tl.constexpr,\n",
    "):\n",
    "    # compute memory offsets of elements handled by this instance\n",
    "    pid = tl.program_id(axis=0)\n",
    "    block_start = pid * BLOCK_SIZE\n",
    "    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n",
    "    # load data from x\n",
    "    mask = offsets < n_elements\n",
    "    x = tl.load(x_ptr + offsets, mask=mask)\n",
    "    # randomly prune it\n",
    "    random = tl.rand(seed, offsets)\n",
    "    x_keep = random > p\n",
    "    # write-back\n",
    "    output = tl.where(x_keep, x / (1 - p), 0.0)\n",
    "    tl.store(output_ptr + offsets, output, mask=mask)\n",
    "\n",
    "\n",
    "def seeded_dropout(x, p, seed):\n",
    "    output = torch.empty_like(x)\n",
    "    assert x.is_contiguous()\n",
    "    n_elements = x.numel()\n",
    "    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']), )\n",
    "    _seeded_dropout[grid](x, output, n_elements, p, seed, BLOCK_SIZE=1024)\n",
    "    return output\n",
    "\n",
    "\n",
    "x = torch.randn(size=(10, )).cuda()\n",
    "# Compare this to the baseline - dropout mask is never instantiated!\n",
    "output = seeded_dropout(x, p=0.5, seed=123)\n",
    "output2 = seeded_dropout(x, p=0.5, seed=123)\n",
    "output3 = seeded_dropout(x, p=0.5, seed=512)\n",
    "\n",
    "print(\n",
    "    tabulate.tabulate([\n",
    "        [\"input\"] + x.tolist(),\n",
    "        [\"output (seed = 123)\"] + output.tolist(),\n",
    "        [\"output (seed = 123)\"] + output2.tolist(),\n",
    "        [\"output (seed = 512)\"] + output3.tolist(),\n",
    "    ]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
